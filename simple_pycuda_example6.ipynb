{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2019, 1, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nMatrix multiplication example\\nmultiply (N x M) matrix and (M x N) martix\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pycuda\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "\n",
    "from pycuda.compiler import SourceModule\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "print(pycuda.VERSION)\n",
    "\n",
    "\"\"\"\n",
    "Matrix multiplication example\n",
    "multiply (N x M) matrix and (M x N) martix\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = np.int32(1024)\n",
    "k = np.int32(256)\n",
    "n = np.int32(512)\n",
    "MAT_SIZE1 = (m, k)\n",
    "MAT_SIZE2 = (k, n)\n",
    "RES_SIZE = (m, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensor and copy it to gpu memory\n",
    "# Randomly initialize a vector\n",
    "a = np.ones(shape=MAT_SIZE1, dtype=np.float32)\n",
    "a = a.astype(np.float32)\n",
    "b = np.ones(shape=MAT_SIZE2, dtype=np.float32)\n",
    "# Allocate memory at device\n",
    "a_gpu = cuda.mem_alloc(a.nbytes)\n",
    "b_gpu = cuda.mem_alloc(b.nbytes)\n",
    "# Allocate same size of memory for result vector\n",
    "result = np.zeros(shape=RES_SIZE, dtype=np.float32)\n",
    "result_gpu = cuda.mem_alloc(result.nbytes)\n",
    "# Copy the cpu vector to gpu\n",
    "cuda.memcpy_htod(a_gpu, a)\n",
    "cuda.memcpy_htod(b_gpu, b)\n",
    "\n",
    "# Copy matirx information to gpu mem\n",
    "m_gpu = cuda.mem_alloc(4)\n",
    "k_gpu = cuda.mem_alloc(4)\n",
    "n_gpu = cuda.mem_alloc(4)\n",
    "\n",
    "cuda.memcpy_htod(m_gpu, m)\n",
    "cuda.memcpy_htod(k_gpu, k)\n",
    "cuda.memcpy_htod(n_gpu, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2,  3,  4,  5,  6,  7],\n",
       "       [ 8,  9, 10, 11, 12, 13, 14, 15],\n",
       "       [16, 17, 18, 19, 20, 21, 22, 23],\n",
       "       [24, 25, 26, 27, 28, 29, 30, 31]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(range(32)).reshape(4,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2,  3],\n",
       "       [ 4,  5,  6,  7],\n",
       "       [ 8,  9, 10, 11],\n",
       "       [12, 13, 14, 15],\n",
       "       [16, 17, 18, 19],\n",
       "       [20, 21, 22, 23],\n",
       "       [24, 25, 26, 27],\n",
       "       [28, 29, 30, 31]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(range(32)).reshape(8,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Square matrix m x k , k x n = m x n matrix\n",
    "mod = SourceModule(\"\"\"\n",
    "    __global__ void matMul2D(float *a, float *b, float *res, int m, int k, int n)\n",
    "    {\n",
    "        // get row, col idx\n",
    "        int rowIdx = blockDim.x * blockIdx.x + threadIdx.x;\n",
    "        int colIdx = blockDim.y * blockIdx.y + threadIdx.y;\n",
    "        \n",
    "        // one thread per each pixels\n",
    "        // I think it should be optimized\n",
    "        if (rowIdx < m && colIdx < n)\n",
    "        {\n",
    "            int element = 0;\n",
    "            for (int i=0; i<k; i++)\n",
    "            {\n",
    "                element += a[rowIdx*k+i] * b[i*n+colIdx];\n",
    "            }\n",
    "            res[rowIdx*n+colIdx] = element;\n",
    "        }\n",
    "    }\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for gpu operation :  0.0010352134704589844\n"
     ]
    }
   ],
   "source": [
    "# Let's compare operating time\n",
    "# GPU\n",
    "startTime = time.time()\n",
    "func = mod.get_function(\"matMul2D\")\n",
    "func(a_gpu, b_gpu, result_gpu, m, k, n, block=(16,16,1), grid =(int(m/16),int(n/16)))\n",
    "consumedTime = time.time() - startTime\n",
    "print(\"Time for gpu operation : \", consumedTime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for cpu operation :  0.015708208084106445\n"
     ]
    }
   ],
   "source": [
    "# CPU\n",
    "startTime = time.time()\n",
    "result_cpu = np.dot(a, b)\n",
    "consumedTime = time.time() - startTime\n",
    "print(\"Time for cpu operation : \", consumedTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the result from device to host\n",
    "#result = np.zeros(shape=(4), dtype=np.float32)\n",
    "cuda.memcpy_dtoh(result, result_gpu)\n",
    "a_gpu.free()\n",
    "b_gpu.free()\n",
    "result_gpu.free()\n",
    "m_gpu.free()\n",
    "n_gpu.free()\n",
    "k_gpu.free()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is it same? :  True\n"
     ]
    }
   ],
   "source": [
    "# Compare the results\n",
    "print(\"Is it same? : \", (result == result_cpu).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[256., 256., 256., ..., 256., 256., 256.],\n",
       "       [256., 256., 256., ..., 256., 256., 256.],\n",
       "       [256., 256., 256., ..., 256., 256., 256.],\n",
       "       ...,\n",
       "       [256., 256., 256., ..., 256., 256., 256.],\n",
       "       [256., 256., 256., ..., 256., 256., 256.],\n",
       "       [256., 256., 256., ..., 256., 256., 256.]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[256., 256., 256., ..., 256., 256., 256.],\n",
       "       [256., 256., 256., ..., 256., 256., 256.],\n",
       "       [256., 256., 256., ..., 256., 256., 256.],\n",
       "       ...,\n",
       "       [256., 256., 256., ..., 256., 256., 256.],\n",
       "       [256., 256., 256., ..., 256., 256., 256.],\n",
       "       [256., 256., 256., ..., 256., 256., 256.]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
