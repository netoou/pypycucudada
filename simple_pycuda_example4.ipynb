{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2019, 1, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nMatrix multiplication example\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pycuda\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "\n",
    "from pycuda.compiler import SourceModule\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "print(pycuda.VERSION)\n",
    "\n",
    "\"\"\"\n",
    "Matrix multiplication example\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAT_SIZE = (16, 16) # row, col\n",
    "BLOCKSIZE = 4\n",
    "GRIDSIZE = int(np.ceil(256/BLOCKSIZE))\n",
    "THREAD_PER_BLOCK = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensor and copy it to gpu memory\n",
    "# Randomly initialize a vector\n",
    "#a = np.ones(shape=(64*64), dtype=np.float32)\n",
    "a = np.ones(shape=MAT_SIZE, dtype=np.float32)\n",
    "a = a.astype(np.float32)\n",
    "b = np.ones(shape=MAT_SIZE, dtype=np.float32)\n",
    "# Allocate memory at device\n",
    "a_gpu = cuda.mem_alloc(a.nbytes)\n",
    "b_gpu = cuda.mem_alloc(b.nbytes)\n",
    "# Allocate same size of memory for result vector\n",
    "result = np.zeros(shape=MAT_SIZE, dtype=np.float32)\n",
    "result_gpu = cuda.mem_alloc(result.nbytes)\n",
    "# Copy the cpu vector to gpu\n",
    "cuda.memcpy_htod(a_gpu, a)\n",
    "cuda.memcpy_htod(b_gpu, b)\n",
    "# width and height\n",
    "w = np.array([16], dtype=np.int)\n",
    "h = np.array([16], dtype=np.int)\n",
    "width = cuda.mem_alloc(w.nbytes)\n",
    "height = cuda.mem_alloc(h.nbytes)\n",
    "\n",
    "cuda.memcpy_htod(width, w)\n",
    "cuda.memcpy_htod(height, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = SourceModule(\"\"\"\n",
    "    __global__ void matMul2D(float *a, float *b, float *res, int *width, int *height)\n",
    "    {\n",
    "        int tx = blockDim.x * blockIdx.x + threadIdx.x;\n",
    "        int ty = blockDim.y * blockIdx.y + threadIdx.y;\n",
    "        int tid = width[0] * ty + tx;\n",
    "        \n",
    "        int value = 0;\n",
    "        int Mval = 0;\n",
    "        int Nval = 0;\n",
    "        \n",
    "        for (int i=0; i<width[0]; i++)\n",
    "        {\n",
    "            Mval = a[ty*width[0]+i];\n",
    "            Nval = b[i*width[0]+tx];\n",
    "            value += Mval * Nval;\n",
    "        }\n",
    "        \n",
    "        res[tid] = value;\n",
    "    }\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for gpu operation :  0.0006635189056396484\n"
     ]
    }
   ],
   "source": [
    "# Let's compare operating time\n",
    "# GPU\n",
    "startTime = time.time()\n",
    "func = mod.get_function(\"matMul2D\")\n",
    "func(a_gpu, b_gpu, result_gpu, width, height, block=(16,16,1), grid =(1,1))\n",
    "consumedTime = time.time() - startTime\n",
    "print(\"Time for gpu operation : \", consumedTime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for cpu operation :  0.0008044242858886719\n"
     ]
    }
   ],
   "source": [
    "# CPU\n",
    "startTime = time.time()\n",
    "result_cpu = np.dot(a, b)\n",
    "consumedTime = time.time() - startTime\n",
    "print(\"Time for cpu operation : \", consumedTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the result from device to host\n",
    "#result = np.zeros(shape=(4), dtype=np.float32)\n",
    "cuda.memcpy_dtoh(result, result_gpu)\n",
    "a_gpu.free()\n",
    "b_gpu.free()\n",
    "result_gpu.free()\n",
    "width.free()\n",
    "height.free()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is it same? :  True\n"
     ]
    }
   ],
   "source": [
    "# Compare the results\n",
    "print(\"Is it same? : \", (result == result_cpu).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "        16., 16., 16.],\n",
       "       [16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "        16., 16., 16.],\n",
       "       [16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "        16., 16., 16.],\n",
       "       [16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "        16., 16., 16.],\n",
       "       [16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "        16., 16., 16.],\n",
       "       [16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "        16., 16., 16.],\n",
       "       [16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "        16., 16., 16.],\n",
       "       [16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "        16., 16., 16.],\n",
       "       [16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "        16., 16., 16.],\n",
       "       [16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "        16., 16., 16.],\n",
       "       [16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "        16., 16., 16.],\n",
       "       [16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "        16., 16., 16.],\n",
       "       [16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "        16., 16., 16.],\n",
       "       [16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "        16., 16., 16.],\n",
       "       [16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "        16., 16., 16.],\n",
       "       [16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "        16., 16., 16.]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "        16., 16., 16.],\n",
       "       [16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "        16., 16., 16.],\n",
       "       [16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "        16., 16., 16.],\n",
       "       [16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "        16., 16., 16.],\n",
       "       [16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "        16., 16., 16.],\n",
       "       [16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "        16., 16., 16.],\n",
       "       [16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "        16., 16., 16.],\n",
       "       [16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "        16., 16., 16.],\n",
       "       [16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "        16., 16., 16.],\n",
       "       [16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "        16., 16., 16.],\n",
       "       [16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "        16., 16., 16.],\n",
       "       [16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "        16., 16., 16.],\n",
       "       [16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "        16., 16., 16.],\n",
       "       [16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "        16., 16., 16.],\n",
       "       [16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "        16., 16., 16.],\n",
       "       [16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "        16., 16., 16.]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
